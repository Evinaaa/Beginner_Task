# task1

## 处理流程：

读取文本->提取词向量（BOW，N-gram）->softmax回归->输出预测特征

## 实验设置：

不同的特征：set-of-word，bag-of-word,N-gram(2-gram)

不同的学习率[0.0001,0.001,0.01,0.1,1,10,100]

梯度下降方法：随机梯度下降，batch下降和mini-batch

## 实验结果：

![task1.png](task1%2023485519f85880589f10ccf4372eed75/task1.png)

整体来看，测试集和训练集准确率分布和趋势是一致对，训练集精度高于测试集

对于不同特征的提取方法，我们可以看到N元特征明显要优于词袋模型，这是因为N元特征考虑了词序。

对于不同的学习率，小的学习率并未让模型收敛，而大的学习率相比之下会更好。

对于不同的梯度下降方法，mini-batch和batch在学习率较小时表现趋势几乎一致，然而在大的学习率下，mini-batch有着更好的表现。shuffle在学习率较小时明显比其他两种方法准确率高得多，在大的学习率下和mini-batch准确率相差不大，但依然是最优的，整体看来batch表现最差。