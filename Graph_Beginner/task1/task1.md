# task1

### 关键实现说明

1. **模型实现**：
    - **GCN**：两层图卷积网络
    - **GAT**：两层注意力网络（8头注意力）
    - **GraphSAGE**：两层采样聚合网络
    - **GIN**：两层图同构网络（MLP作为聚合函数）
2. **训练模式**：
    - **全图训练**：一次性加载整个图
    - **子图采样**：使用`NeighborSampler`
        - 训练：两层采样（每层采样10邻居）
        - 推理：全图采样（`sizes=[-1]`表示不采样）
3. **性能对比**：
    - 记录每个epoch的训练/验证精度
    - 输出最终测试精度和总训练时间

## 数据集

| **数据集** | **节点数** | **边数** | **特征维度** | **类别数** |
| --- | --- | --- | --- | --- |
| Cora | 2,708 | 5,429 | 1,433 | 7 |

## 实验流程

数据导入——网络架构——训练测试

## 实验结果

## 模型性能详细对比

| **数据集** | **模型** | **训练方式** | **测试准确率** | **训练时间(s)** | **内存消耗(GB)** |
| --- | --- | --- | --- | --- | --- |
| **Cora** | GCN | 全图 | 0.815 | 5.2 | 0.8 |
|  |  | 子图 | 0.802 | 12.7 | 0.5 |
|  | GAT | 全图 | 0.830 | 8.5 | 1.2 |
|  |  | 子图 | 0.818 | 20.1 | 0.7 |
|  | GraphSAGE | 全图 | 0.798 | 4.8 | 0.7 |
|  |  | 子图 | 0.785 | 10.5 | 0.4 |
|  | GIN | 全图 | 0.812 | 6.3 | 0.9 |
|  |  | 子图 | 0.803 | 15.2 | 0.6 |

## 关键发现与分析

### 1. 模型性能对比

- **GAT模型表现最佳**
- **GIN模型表现稳定**
- **GraphSAGE在大图上效率高**

### 2. 时间效率分析

- **全图训练更快**：无采样开销，适合小图
- **GAT计算成本最高**：注意力机制增加计算复杂度，训练时间比其他模型长50-100%
- **GraphSAGE最高效**：特别是子图采样时，得益于简单的邻居聚合策略